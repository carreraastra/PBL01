{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">Image Matching Challenge 2022 - EDA</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üõë &nbsp; WARNING:</b><br><br><b>THIS IS A WORK IN PROGRESS</b><br>\n</div></center>\n\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n</div></center>\n\n\n","metadata":{"papermill":{"duration":0.120561,"end_time":"2021-11-06T21:15:09.611563","exception":false,"start_time":"2021-11-06T21:15:09.491002","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#dataset_exploration\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION & PREPROCESSING</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model_baseline\">5&nbsp;&nbsp;&nbsp;&nbsp;BASELINE</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#submission\">6&nbsp;&nbsp;&nbsp;&nbsp;SUBMISSION</a></h3>\n\n---","metadata":{"papermill":{"duration":0.085591,"end_time":"2021-11-06T21:15:09.78303","exception":false,"start_time":"2021-11-06T21:15:09.697439","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: black;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{"papermill":{"duration":0.050527,"end_time":"2021-11-06T21:15:09.894476","exception":false,"start_time":"2021-11-06T21:15:09.843949","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_hub as tfhub; print(f\"\\t\\t‚Äì TENSORFLOW HUB VERSION: {tfhub.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom scipy.spatial import cKDTree\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"papermill":{"duration":162.144149,"end_time":"2021-11-06T21:17:52.087371","exception":false,"start_time":"2021-11-06T21:15:09.943222","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{"papermill":{"duration":0.05019,"end_time":"2021-11-06T21:17:52.231372","exception":false,"start_time":"2021-11-06T21:17:52.181182","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION</b>\n\n**Participants are asked to estimate the relative pose of one image with respect to another. For each ID in the test set, you must predict the fundamental matrix between the two views.**\n\nThe classical pipeline (also implemented in examples) is to\n1. Extract Local Features\n2. Match Local Features in Image 1 to Local Features in Image 2\n3. Filter These Feature Matches\n4. Apply RANSAC (Or Some Such Similar Outlier Elimination)\n\nYou can train whole pipeline end-to-end, which is hard, or train in modular way...\n1. DISK for local features\n2. SuperGlue for matching\n3. OANet for filtering\n4. Apply some algorithmic improvements to RANSAC, to have better results.\n\n<a href=\"https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/318022\">[REF]</a>\n\n**The Basic Pipeline - NOTE: This Competition is Task 1**\n\n<center><img src=\"https://ducha-aiki.github.io/wide-baseline-stereo-blog/images/copied_from_nb/2021-05-14-IMC2020-competition-recap_files/att_00000.png\"></center>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">CONTEXT</b>\n\nFor most of us, our best camera is part of the phone in our pocket. We may take a snap of a landmark, like the Trevi Fountain in Rome, and share it with friends. By itself, that photo is two-dimensional and only includes the perspective of our shooting location. Of course, a lot of people have taken photos of that fountain. Together, we may be able to create a more complete, three-dimensional view. What if machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet?\n\nThe process to reconstruct 3D objects and buildings from images is called Structure-from-Motion (SfM). Typically, these images are captured by skilled operators under controlled conditions, ensuring homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, lighting and weather conditions, occlusions from people and vehicles, and even user-applied filters.\n\n<center><img src=\"https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/trevi-canvas-licensed-nonoderivs.jpg\"></center>\n\n<br>\n\n**The first part of the problem is to identify which parts of two images capture the same physical points of a scene, such as the corners of a window. This is typically achieved with local features (key locations in an image that can be reliably identified across different views). Local features contain short description vectors that capture the appearance around the point of interest. By comparing these descriptors, likely correspondences can be established between the pixel coordinates of image locations across two or more images. This ‚Äúimage registration‚Äù makes it possible to recover the 3D location of the point by triangulation.**\n\n<br>\n\nGoogle employs Structure-from-Motion techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic, and better leverage the volume of data already publicly available, Google presents this competition in collaboration with the University of British Columbia and Czech Technical University.\n\n<center><img src=\"https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/image3.gif\"></center>\n\nIn this code competition, you‚Äôll create a machine learning algorithm that registers two images from different viewpoints. With access to a dataset of thousands of images to train and test your model, top-scoring notebooks will do so with the most accuracy.\n\nIf successful, you'll help solve this well-known problem in computer vision, making it possible to map the world with unstructured image collections. Your solutions will have applications in photography and cultural heritage preservation, along with Google Maps. Winners will also be invited to give a presentation as part of the Image Matching: Local Features and Beyond workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) in June.\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">MORE BACKGROUND INFORMATION</b>\n\nThere is lots... I will fill this in later.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">WELCOME MESSAGE FROM HOST</b>\n\nOn behalf of the organizers, I would like to welcome you to the challenge! We've had three editions before (outside Kaggle), and It's a bit different than most of the problems usually presented here, so we would like to provide you with a few links to get you started:\n* <a href=\"https://image-matching-workshop.github.io/\">Link to the workshop page.</a>\n* <a href=\"https://www.cs.ubc.ca/research/image-matching-challenge/current/\">Link to the previous version of the challenge</a> (outside Kaggle).\n* <a href=\"https://arxiv.org/abs/2003.01587\">IJCV paper</a>: a paper we published on this problem/data.\n\nAnd some example notebooks:\n\n* <a href=\"https://www.kaggle.com/eduardtrulls/imc2022-training-data\">Loading and inspecting the training data.</a>\n* <a href=\"https://www.kaggle.com/eduardtrulls/imc2022-training-set-eval-one-function\">Evaluating a csv file on the training data with a single function.</a>\n* <a href=\"https://www.kaggle.com/code/eduardtrulls/imc2022-baseline-submission-sift\">Creating a simple submission on CPU with OpenCV</a>.\n* Creating a simple submission on GPU with the <a href=\"https://kornia.github.io/\">Kornia library</a> (<a href=\"https://www.kaggle.com/code/oldufo/imc2022-baseline-submission-kornia\">notebook</a>) or <a href=\"https://arxiv.org/abs/2006.13566\">DISK</a> (<a href=\"https://www.kaggle.com/eduardtrulls/imc2022-baseline-submission-disk\">notebook</a>). Please see <a href=\"https://www.kaggle.com/code/eduardtrulls/imc2022-dependencies\">this notebook to install dependencies into offline notebooks.\n\nIn addition to the Competition Data provided, you may want to find and use additional training data (for instance, a data set with tens of thousands of posed images including pixel-to-pixel mappings, such as the <a href=\"https://www.cs.ubc.ca/~kmyi/imw2020/data.html\">PhotoTourism dataset</a> made available by the University of British Columbia), to the extent you have the necessary rights to use it in connection with this Competition.","metadata":{"papermill":{"duration":0.053372,"end_time":"2021-11-06T21:17:52.337029","exception":false,"start_time":"2021-11-06T21:17:52.283657","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">1.2 COMPETITION EVALUATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n\nParticipants are asked to estimate the relative pose of one image with respect to another. \n* Submissions are evaluated on the **m**ean **A**verage **A**ccuracy (**mAA**) of the estimated poses. \n* Given a fundamental matrix and the hidden ground truth, we compute the error in terms of rotation and translation:\n    * **rotation** --> **ùúñùëÖ** (in degrees) \n    * **translation** --> **ùúñùëá** (in meters)\n* Given one threshold over each, we classify a pose as accurate if it meets both thresholds. We do this over ten pairs of thresholds, one pair at a time\n    * i.e. at 1<sup>ùëú</sup> and 20 cm for the finest level\n    * i.e. at 10<sup>ùëú</sup> and 5 m for the coarsest level\n```python\nthresholds_r = np.linspace(1, 10, 10)  # In degrees.\nthresholds_t = np.geomspace(0.2, 5, 10)  # In meters.\n```\n* We then calculate the percentage of image pairs that meet every pair of thresholds, and average the results over all thresholds, which rewards more accurate poses. \n* As the dataset contains multiple scenes, which may have a different number of pairs, **we compute this metric separately for each scene and average it afterwards**. \n* A python implementation of this metric is available on <a href=\"https://www.kaggle.com/code/eduardtrulls/imc2022-tutorial-load-and-evaluate-training-data\">this notebook</a>.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FILE</b>\n\nFor each ID in the test set, you must predict the fundamental matrix between the two views. The file should contain a header and have the following format:\n\n```\nsample_id,fundamental_matrix\na;b;c-d,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\na;b;e-f,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\na;b;g-h,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\netc\n```\n\nNote that **`fundamental_matrix`** is a **`3√ó3`** matrix, flattened into a vector in row-major order.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase; color: red;\">IS THIS A CODE COMPETITION?</b>\n\n<font style=\"color:red; font-weight: bold; font-size: 20px;\">YES!</font>\n","metadata":{"papermill":{"duration":0.052259,"end_time":"2021-11-06T21:17:52.443218","exception":false,"start_time":"2021-11-06T21:17:52.390959","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">1.3 OTHER NOTABLE THINGS</h3>\n\n---\n\nThis competition is part of the <a href=\"https://image-matching-workshop.github.io/\">Image Matching: Local Features and Beyond</a> workshop at <a href=\"https://cvpr2022.thecvf.com/\">CVPR'22</a>. Selected submissions to the competition will be invited to give talks at the workshop on June 20, 2022 in New Orleans, USA. Attending the workshop is not required to participate in the competition, but only teams that are attending the workshop will be considered to present their work.\n\nCVPR 2022 will be a hybrid conference. Attendees presenting in person are responsible for all costs associated with expenses and fees to attend CVPR 2022.","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">1.4 DATASET OVERVIEW</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n\nAligning photographs of the same scene is a problem of longstanding interest to computer vision researchers. Your challenge in this competition is to generate mappings between pairs of photos from various cities.\n\nThis competition uses a hidden test (n~=10_000). \n* When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">FILE INFORMATION</b>\n\n<code style=\"font-weight: bold; font-size: 14px;\">train/*/calibration.csv</code>\n* **`image_id`**\n    * The image filename.\n* **`camera_intrinsics`** \n    * The 3√ó3 calibration matrix ùêä for this image, flattened into a vector by row-major indexing.\n* **`rotation_matrix`**\n    * The 3√ó3 rotation matrix ùêë for this image, flattened into a vector by row-major indexing.\n* **`translation_vector`**\n    * The translation vector ùêì.\n\n<code style=\"font-weight: bold; font-size: 14px;\">train/*/pair_covisibility.csv</code>\n* **`pair`**\n    * A string identifying a pair of images, encoded as two image filenames (without the extension) separated by a hyphen, as **`key1-key2`**, where **`key1 > key2`**.\n* **`covisibility`**\n    * An estimate of the overlap between the two images. \n    * Higher numbers indicate greater overlap. \n    * We recommend using all pairs with a covisibility estimate of 0.1 or above. \n    * The procedure used to derive this number is described in Section 3.2 and Figure 5 of <a href=\"https://arxiv.org/pdf/2003.01587.pdf\">this paper</a>.\n* **`fundamental_matrix1`**\n    * **The target column** as derived from the calibration files. \n    * Please see the <a href=\"https://www.kaggle.com/competitions/image-matching-challenge-2022/overview/problem-definition\">problem definition page</a> for more details.\n\n<code style=\"font-weight: bold; font-size: 14px;\">train/scaling_factors.csv</code> \n* The poses for each scene where reconstructed via <a href=\"https://en.wikipedia.org/wiki/Structure_from_motion\">Structure-from-Motion</a>, and are only accurate up to a scaling factor. \n* This file contains a scalar for each scene which can be used to convert them to meters. \n* For code examples, please refer to <a href=\"https://www.kaggle.com/eduardtrulls/imc2022-tutorial-load-and-evaluate-training-data\">this notebook</a>.\n\n<code style=\"font-weight: bold; font-size: 14px;\">train/*/images/</code>\n* A batch of images all taken near the same location.\n\n<code style=\"font-weight: bold; font-size: 14px;\">train/LICENSE.txt</code>\n* Records of the specific source of and license for each image.\n\n<code style=\"font-weight: bold; font-size: 14px;\">sample_submission.csv</code>\n* A valid sample submission.\n* **`sample_id`**\n    * The unique identifier for the image pair.\n* **`fundamental_matrix`**\n    * The target column. Please see the <a href=\"https://www.kaggle.com/competitions/image-matching-challenge-2022/overview/problem-definition\">problem definition page</a> for more details. \n    * The default values are randomly generated.\n\n<code style=\"font-weight: bold; font-size: 14px;\">test.csv</code> \n* Expect to see roughly 10,000 pairs of images in the hidden test set.\n* **`sample_id`**\n    * The unique identifier for the image pair.\n* **`batch_id`**\n    * The batch ID.\n* **`image_[1/2]_id`**\n    * The filenames of each image in the pair.\n\n<code style=\"font-weight: bold; font-size: 14px;\">test_images</code> \nThe test set. \n* The test data comes from a different source than the train data and contains photos of mostly urban scenes with variable degrees of overlap. \n* The two images forming a pair may have been collected months or years apart, but **never less than 24 hours.**\n* Bridging this domain gap is part of the competition. \n* The images have been resized so that the **longest edge is around 800 pixels**, **may have different aspect ratios (including portrait and landscape), and are upright**.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">DISCOVERED  INFORMATION [TENTATIVE]</b>\n\nTBD","metadata":{"papermill":{"duration":0.052579,"end_time":"2021-11-06T21:17:52.548406","exception":false,"start_time":"2021-11-06T21:17:52.495827","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{"papermill":{"duration":0.052944,"end_time":"2021-11-06T21:17:52.65284","exception":false,"start_time":"2021-11-06T21:17:52.599896","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n\n---\n\nIn order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n\n1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n\n<br>\n\n```python\n# The name you gave to the TPU to use\nTPU_WORKER = 'my-tpu-name'\n\n# or you can also specify the grpc path directly\n# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n\n# The zone you chose when you created the TPU to use on GCP.\nZONE = 'us-east1-b'\n\n# The name of the GCP project where you created the TPU to use on GCP.\nPROJECT = 'my-tpu-project'\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCES:</b><br><br>\n    - <a href=\"https://www.tensorflow.org/guide/tpu#tpu_initialization\"><b>Guide - Use TPUs</b></a><br>\n    - <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\"><b>Doc - TPUClusterResolver</b></a><br>\n\n</div>","metadata":{"papermill":{"duration":0.053821,"end_time":"2021-11-06T21:17:52.761303","exception":false,"start_time":"2021-11-06T21:17:52.707482","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"papermill":{"duration":0.07574,"end_time":"2021-11-06T21:17:52.892074","exception":false,"start_time":"2021-11-06T21:17:52.816334","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n</div>","metadata":{"papermill":{"duration":0.053551,"end_time":"2021-11-06T21:17:52.999073","exception":false,"start_time":"2021-11-06T21:17:52.945522","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('image-matching-challenge-2022')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/image-matching-challenge-2022\"\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"papermill":{"duration":0.0797,"end_time":"2021-11-06T21:17:53.133972","exception":false,"start_time":"2021-11-06T21:17:53.054272","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n\n---\n\n\n**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n\n<br>\n\nWhen a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n\nXLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; NOTE:</b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCE:</b><br><br>    - <a href=\"https://www.tensorflow.org/xla\"><b>XLA: Optimizing Compiler for Machine Learning</b></a><br>\n</div>","metadata":{"papermill":{"duration":0.051494,"end_time":"2021-11-06T21:17:53.236017","exception":false,"start_time":"2021-11-06T21:17:53.184523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"papermill":{"duration":0.128128,"end_time":"2021-11-06T21:17:53.442803","exception":false,"start_time":"2021-11-06T21:17:53.314675","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{"papermill":{"duration":0.090507,"end_time":"2021-11-06T21:17:53.624612","exception":false,"start_time":"2021-11-06T21:17:53.534105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN META ...\\n\")\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nSCALING_CSV = os.path.join(TRAIN_DIR, \"scaling_factors.csv\")\nscaling_df = pd.read_csv(SCALING_CSV)\nscale_map = scaling_df.groupby(\"scene\")[\"scaling_factor\"].first().to_dict()\n\n# 'british_museum', 'piazza_san_marco', \n# 'trevi_fountain', 'st_pauls_cathedral', \n# 'colosseum_exterior', 'buckingham_palace', \n# 'temple_nara_japan', 'sagrada_familia', \n# 'grand_place_brussels', 'pantheon_exterior', \n# 'notre_dame_front_facade', 'st_peters_square', \n# 'sacre_coeur', 'taj_mahal', \n# 'lincoln_memorial_statue', 'brandenburg_gate'\nTRAIN_SCENES = scaling_df.scene.unique().tolist()\n\ntrain_map = {} \nfor _s in tqdm(TRAIN_SCENES, total=len(TRAIN_SCENES)):\n    # Initialize    \n    train_map[_s] = {}\n    \n    # Image Stuff\n    train_map[_s][\"images\"] = sorted(tf.io.gfile.glob(os.path.join(TRAIN_DIR, _s, \"images\", \"*.jpg\")))\n    train_map[_s][\"image_ids\"] = [_f_path[:-4].rsplit(\"/\", 1)[-1] for _f_path in train_map[_s][\"images\"]]\n    \n    # Calibration Stuff (CAL)\n    _tmp_cal_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"calibration.csv\"))\n    _tmp_cal_df[\"image_path\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_cal_df[\"image_id\"]+\".jpg\"\n    train_map[_s][\"cal_df\"]=_tmp_cal_df.copy()\n        \n    # Pair Covisibility Stuff (PCO)\n    _tmp_pco_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"pair_covisibility.csv\"))\n    _tmp_pco_df[\"image_id_1\"], _tmp_pco_df[\"image_id_2\"] = zip(*_tmp_pco_df.pair.apply(lambda x: x.split(\"-\")))\n    _tmp_pco_df[\"image_path_1\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_1\"]+\".jpg\"\n    _tmp_pco_df[\"image_path_2\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_2\"]+\".jpg\"\n    train_map[_s][\"pco_df\"] = _tmp_pco_df.copy()\n\n#cleanup\ndel _tmp_cal_df, _tmp_pco_df; gc.collect(); gc.collect();\n    \nprint(\"\\n... TEST META ...\\n\")\nTEST_IMG_DIR = os.path.join(DATA_DIR, \"test_images\")\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\ntest_df[\"f_path_1\"] = TEST_IMG_DIR+\"/\"+test_df.batch_id+\"/\"+test_df.image_1_id+\".png\"\ntest_df[\"f_path_2\"] = TEST_IMG_DIR+\"/\"+test_df.batch_id+\"/\"+test_df.image_2_id+\".png\"\ndisplay(test_df)\n\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\ndisplay(ss_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{"papermill":{"duration":0.054893,"end_time":"2021-11-06T21:17:54.695576","exception":false,"start_time":"2021-11-06T21:17:54.640683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    nested_list = [x if type(x) is list else [x,] for x in nested_list]\n    return [item for sublist in nested_list for item in sublist]\n\ndef plot_two_paths(f_path_1, f_path_2, _titles=None):\n    plt.figure(figsize=(20, 10))\n    \n    plt.subplot(1,2,1)\n    if _titles is None:\n        plt.title(f_path_1, fontweight=\"bold\")\n    else:\n        plt.title(_titles[0], fontweight=\"bold\")\n    plt.imshow(cv2.imread(f_path_1)[..., ::-1])\n    plt.axis(False)\n    \n    plt.subplot(1,2,2)\n    if _titles is None:\n        plt.title(f_path_2, fontweight=\"bold\")\n    else:\n        plt.title(_titles[1], fontweight=\"bold\")\n\n    plt.imshow(cv2.imread(f_path_2)[..., ::-1])\n    plt.axis(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \ndef pco_random_plot(_df):\n    df_row = _df.sample(1).reset_index(drop=True)\n    plot_two_paths(\n        df_row.image_path_1[0],\n        df_row.image_path_2[0],\n        [f\"Image ID: {df_row.image_id_1} - COV={df_row.covisibility[0]}\",\n         f\"Image ID: {df_row.image_id_2} - COV={df_row.covisibility[0]}\",]\n    )\n    \ndef arr_from_str(_s):\n    return np.fromstring(_s, sep=\" \").reshape((-1,3)).squeeze()\n\ndef get_id2cal_map(_train_map):\n    _id2cal_map = {}\n    for _s in tqdm(TRAIN_SCENES, total=len(TRAIN_SCENES)):\n        for _, _row in _train_map[_s][\"cal_df\"].iterrows():\n            _img_id = _row[\"image_id\"]; _id2cal_map[_img_id]={}\n            _id2cal_map[_img_id][\"camera_intrinsics\"] = arr_from_str(_row[\"camera_intrinsics\"])\n            _id2cal_map[_img_id][\"rotation_matrix\"] = arr_from_str(_row[\"rotation_matrix\"])\n            _id2cal_map[_img_id][\"translation_vector\"] = arr_from_str(_row[\"translation_vector\"])\n            _id2cal_map[_img_id][\"image_path\"] = _row[\"image_path\"]\n    return _id2cal_map\n\nid2cal_map = get_id2cal_map(train_map)","metadata":{"papermill":{"duration":0.098071,"end_time":"2021-11-06T21:17:54.848036","exception":false,"start_time":"2021-11-06T21:17:54.749965","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The following functions are for feature identification, matrix manipulations, etc**","metadata":{}},{"cell_type":"code","source":"def extract_sift_features(rgb_image, detector, n_features):\n    \"\"\" \n    Helper Function to Compute SIFT features for a Given Image\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    # Convert RGB image to Grayscale\n    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n    \n    # Run detector and retrieve only the top keypoints and descriptors \n    kp, desc = detector.detectAndCompute(gray, None)\n    \n    return kp[:n_features], desc[:n_features]\n\ndef extract_keypoints(rgb_image, detector):\n    \"\"\" \n    Helper Function to Compute SIFT features for a Given Image\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    # Convert RGB image to Grayscale\n    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n    \n    # Run detector and retrieve the keypoints\n    return detector.detect(gray)\n\ndef build_composite_image(im1, im2, axis=1, margin=0, background=1):\n    \"\"\"\n    Convenience function to stack two images with different sizes.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n\ndef draw_cv_matches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    \"\"\"\n    Draw keypoints and matches.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    composite, v_offset, h_offset = build_composite_image(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite\n\ndef normalize_keypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\ndef compute_essential_matrix(F, K1, K2, kp1, kp2):\n    \"\"\"\n    Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. \n    Note that we ask participants to estimate F, i.e., without relying on known intrinsics.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n    # We do not account for this here, as the modern RANSACs do not do this:\n    # https://opencv.org/evaluating-opencvs-new-ransacs\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = normalize_keypoints(kp1, K1)\n    kp2n = normalize_keypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\ndef quaternion_from_matrix(matrix):\n    \"\"\"\n    Transform a rotation matrix into a quaternion\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \n    The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\\\n    \"\"\"\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([\n        [m00 - m11 - m22, 0.0, 0.0, 0.0],\n        [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n        [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n        [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]\n    ])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n    if q[0] < 0:\n        np.negative(q, q)\n    return q\n\n\ndef compute_error_for_example(q_gt, T_gt, q, T, scale, eps=1e-15):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. \n    These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"dataset_exploration\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"dataset_exploration\">\n    4&nbsp;&nbsp;DATASET EXPLORATION & PREPROCESSING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n\nLet's investigate the provided information and then plot some images for various things","metadata":{"papermill":{"duration":0.056443,"end_time":"2021-11-06T21:17:54.960518","exception":false,"start_time":"2021-11-06T21:17:54.904075","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.1 THE TRAINING DATA</h3>\n\n---\n\nBefore we can make some simple deductions, we need to combine the data as much as possible so we can view it with a comprehensive lense.","metadata":{}},{"cell_type":"code","source":"_cal_dfs = []\n_pco_dfs = []\n_s_dfs = []\nfor _s in TRAIN_SCENES:\n    _s_map = train_map[_s]\n    _s_dfs.append(pd.DataFrame({\n        \"scene\":[_s,]*len(_s_map[\"images\"]),\n        \"f_path\":_s_map[\"images\"],\n    }))\n    _cal_dfs.append(_s_map[\"cal_df\"])\n    _pco_dfs.append(_s_map[\"pco_df\"])\n\nall_pco_dfs = pd.concat(_pco_dfs).reset_index(drop=True)\nall_cal_dfs = pd.concat(_cal_dfs).reset_index(drop=True)\n\ntrain_df = pd.concat(_s_dfs).reset_index(drop=True)\ntrain_df.insert(0, \"image_id\", train_df.f_path.apply(lambda x: x[:-4].rsplit(\"/\", 1)[-1]))\ntrain_df = pd.merge(train_df, all_cal_dfs, on=\"image_id\").drop(columns=[\"image_path\",])\n\ncov_1_df = all_pco_dfs[[\"image_id_1\", \"covisibility\"]]\ncov_1_df.columns = [\"image_id\", \"covisibility\"]\ncov_2_df = all_pco_dfs[[\"image_id_2\", \"covisibility\"]]\ncov_2_df.columns = [\"image_id\", \"covisibility\"]\ncov_df = pd.concat([cov_1_df,cov_2_df])\nimg_id_2_cov = cov_df.groupby(\"image_id\")[\"covisibility\"].mean().to_dict()\ndel cov_1_df, cov_2_df, cov_df; gc.collect();\n\n# Add a column for average covisibility\n#    - i.e. For a given image, what is the average \n#           covisibility of all of it's respective pairs\ntrain_df.insert(2, \"mean_covisibility\", train_df.image_id.map(img_id_2_cov))\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(train_df, \"scene\", color=\"scene\", title=\"<b>Image Counts By Scene</b>\")\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images</b>\", xaxis_title=\"<b>Scene Identifier</b>\",\n    legend_title=\"<b>Scene Identifier</b>\", showlegend=False,\n)\nfig.show()\n\nfig = px.histogram(train_df, \"mean_covisibility\", color=\"scene\", title=\"<b>Average Covisibility Of Images Coloured By Scene</b>\", )\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Covisibility Bin</b>\", xaxis_title=\"<b>Covisibility (0-1.00)</b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.2 LET'S LOOK INTO THE CAMERA MATRIX</h3>\n\n---\n\n<sub><b><i>REFERENCE --> wikiwand.com/en/Camera_matrix/</i></b></sub><br>\n\nThe calibration matrices for a given camera calculate the camera matrix using the extrinsic and intrinsic parameters. The extrinsic parameters represent a rigid transformation from 3-D world coordinate system to the 3-D camera‚Äôs coordinate system. The intrinsic parameters represent a projective transformation from the 3-D camera‚Äôs coordinates into the 2-D image coordinates.\n\n---\n\nThis is a fancy way of saying that **extrinsics** \n\n* describe the cameras location within the external (outside-the-camera) world \n    * <b>$(x,y,z)$</b>\n* describe the cameras orientation with respect to the external (outside-the-camera) world \n    * <b>$(\\gamma, \\beta, \\alpha)$</b>\n* Note while these are the values that describe the absolute positions/rotations of the camera, it is often useful to represent them as a series of operations that would move a camera from a level position at the origin to whatever respective position the camera exists at. This is where the concepts of the **rotation** and **translation** matrix come from.\n    \nwhile **intrinsics**...\n\n* Describe how a point in the 3D world is mapped to the 2D image plane\n    * focal length\n    * skew/distortion/scale\n    * principal offsets\n    * etc.\n\nWe can take this to the logical conclusion of building a matrix that can fully describe a camera (the union of these two matrices). This new matrix is called **the camera matrix**.\n\n---\n\n\nIn computer vision a camera matrix or (camera) projection matrix is a $3\\times 4$ matrix which describes the mapping of a pinhole camera from 3D points in the world to 2D points in an image.\n\nLet $\\mathbf {x}$  be a representation of a 3D point in homogeneous coordinates (a 4-dimensional vector), and let  $\\mathbf{y}$  be a representation of the image of this point in the pinhole camera (a 3-dimensional vector). Then the following relation holds\n\n$$\n \\mathbf{y} \\sim \\mathbf{C} \\, \\mathbf{x} \n$$\n\nwhere  $\\mathbf{C}$  is the camera matrix and the $\\, \\sim$  sign implies that the left and right hand sides are equal up to a non-zero scalar multiplication. Since the camera matrix  $\\mathbf{C}$  is involved in the mapping between elements of two projective spaces, it too can be regarded as a projective element. \n\n**This means that  $\\mathbf{C}$  has only 11 degrees of freedom since any multiplication by a non-zero scalar results in an equivalent camera matrix.**\n\n---\n\n***I will spare you the derivation and normalization steps... if you're interested they are detailed in the wiki page linked in reference one above.***\n\n---\n\nGiven the mapping produced by a normalized camera matrix (skipped), the resulting normalized image coordinates can be transformed by means of an arbitrary 2D homography. This **includes 2D translations and rotations** as well as **scaling** (**isotropic** and **anisotropic**) but also general 2D perspective transformations. \n\nSuch a transformation can be represented as a $3\\times  3$  matrix  $\\mathbf {H}$  which maps the homogeneous normalized image coordinates  $\\mathbf{y}$  to the homogeneous transformed image coordinates ${\\mathbf  {y}}'$:\n\n$$\n{\\mathbf  {y}}'={\\mathbf  {H}}\\,{\\mathbf  {y}}\n$$\n\nInserting the above expression for the normalized image coordinates in terms of the 3D coordinates gives\n\n$$\n{\\mathbf  {y}}'={\\mathbf  {H}}\\,{\\mathbf  {C}}_N\\,{\\mathbf  {x}}'\n$$\n\nThis produces the most general form of camera matrix (Note we sometimes use $\\mathbf {K}$ instead of $\\mathbf {H}$)\n\n$$\n{\\mathbf  {C}}={\\mathbf  {H}}\\,{\\mathbf  {C}}_N\\,{\\mathbf  {x}}'={\\mathbf  {H} \\left( {\\mathbf {R}} \\, \\middle| \\, {\\mathbf {t}} \\right)}\n$$","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.3 LET'S LOOK INTO THE CAMERA INSTRICS</h3>\n\n---\n\n<sub><b><i>REFERENCE --> https://ksimek.github.io/2013/08/13/intrinsic/</i></b></sub><br><br>\n\nThe intrinsic matrix is parameterized by Hartley and Zisserman as\n\n$$K=\\begin{pmatrix}\nf_x&s&x_0\\\\\n0&f_y&y_0\\\\\n0&0&1\n\\end{pmatrix}$$\n\nEach intrinsic parameter describes a geometric property of the camera. \n\nLet's examine each of these properties in detail.\n\n<br>\n\n**Focal Length** ‚Äì‚Äì $(f_x, f_y)$\n\nThe focal length is the distance between the pinhole and the film (a.k.a. image plane). \n* For reasons... the focal length is measured in pixels. \n* In a true pinhole camera, both $f_x$ and $f_y$ have the same value, which is illustrated as $f$ below.\n    * In the code cell below we verify that for all our examples $f_x = f_y$\n* In practice, $f_x$ and $f_y$ can differ for a number of reasons... however, we will ignore them for now as in our dataset... these two values are always equal (assume perfectly square pixels!)\n\n<center><img src=\"https://ksimek.github.io/img/intrinsic-focal-length.png\"></center>\n\n<br>\n\n**Principal Point Offset** ‚Äì‚Äì $(x_0, y_0)$\n\nThe camera's **\"principal axis\"** is the line perpendicular to the image plane that passes through the pinhole. \n* Its itersection with the image plane is referred to as the **\"principal point\"**, illustrated below.\n\n<center><img src=\"https://ksimek.github.io/img/intrinsic-pp.png\"></center>\n\nThe **\"principal point offset\"** is the location of the principal point relative to **the film's origin**. \n* The exact definition depends on which convention is used for the location of the origin\n* the illustration below assumes it's at the bottom-left of the film.\n\n<center><img src=\"https://ksimek.github.io/img/intrinsic-pp-offset.png\"></center>\n\nIncreasing $x_0$ shifts the pinhole to the right\n\n<center><img src=\"https://ksimek.github.io/img/intrinsic-pp-offset-delta-alt.png\"></center>\n\nThis is equivalent to shifting the film to the left and leaving the pinhole unchanged.\n\n<center><img src=\"https://ksimek.github.io/img/intrinsic-pp-offset-delta.png\"></center>\n\nNotice that the box surrounding the camera is irrelevant, only the pinhole's position relative to the film matters.\n\n<br>\n\n**Axis Skew** ‚Äì‚Äì $(s)$\n\nAxis skew causes shear distortion in the projected image. \n* In our dataset all skew values should be 0... but it's worth noting that some digitization processes can cause nonzero skew\n\n<br>\n\n---\n\nWe can decompose the intrinsic matrix into a sequence of shear, scaling, and translation transformations, corresponding to axis skew, focal length, and principal point offset, respectively:\n\n<center><img src=\"https://i.ibb.co/Gvt4CBx/Screen-Shot-2022-04-11-at-1-38-20-PM.png\"></center>\n\n---\n\nNow that we understand the basics of the camera intrinsic matrix, we can investigate the distribution of the respective values within the provided train dataset. We will first extract the respective information entities from the intrinsics matrix, verify certain assumptions, and then plot the relevant distributions.","metadata":{}},{"cell_type":"code","source":"# NOTE: Our intrinsics matrix is a flat string... NOTE: [idx]\n#       but when reshaped to 3x3 it will map as follows...\n# STRING --> \"TL[0] T[1] TR[2] CL[3] C[4] CR[5] BL[6] B[7] BR[8]\"\n#\n# MATRIX BELOW\n##############\n#            #\n#  TL  T  TR #\n#  CL  C  CR #\n#  BL  B  BR #\n#            #\n##############\n# \n# Therefore we just index into the string and convert to a digit to access certain elements\ntrain_df[\"fx\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[0])) # 0 = TL \ntrain_df[\"fy\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[4])) # 4 = C\ntrain_df[\"x0\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[2])) # 0 = TR\ntrain_df[\"y0\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[5])) # 4 = CR\ntrain_df[\"s\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[1])) # 1 = TC\n\nif (train_df[\"fx\"]!=train_df[\"fy\"]).sum()==0:\n    print(\"All fx values (focal length x) are equal to the respective fy values (focal length y)... as expected\")\n    \nif train_df[\"s\"].sum()==0:\n    print(\"All skew values are 0... as expected\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(train_df, \"x0\", title=\"<b>Principal Point Offset [x<sub>0</sub>] Coloured By Scene</b>\", color=\"scene\", log_y=False)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per x<sub>0</sub> Bin</b>\", xaxis_title=\"<b>Principal Point Offset [x<sub>0</sub>]</b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\nfig = px.histogram(train_df, \"y0\", title=\"<b>Principal Point Offset [y<sub>0</sub>] Coloured By Scene</b>\", color=\"scene\", log_y=False)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per y<sub>0</sub> Bin</b>\", xaxis_title=\"<b>Principal Point Offset [y<sub>0</sub>]</b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n# We use \"fx\" because all \"fx\" values are equivalent to \"fy\" values\nfig = px.histogram(train_df, \"fx\", title=\"<b>Focal Lengths Coloured By Scene</b>\", color=\"scene\", log_x=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Focal Length Bin</b>\", xaxis_title=\"<b>Focal Length <i>(log scale)</i></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.4 LET'S LOOK INTO THE CAMERA EXTRINSICS</h3>\n\n---\n\n<sub><b><i>REFERENCE --> https://ksimek.github.io/2012/08/22/extrinsic/</i></b></sub><br><br>\n \nAs discussed previously, the camera's **extrinsic matrix** describes the **camera's location in the world (3D), and what direction it's pointing (orientation)**. It has two components: \n* **a rotation matrix, $\\mathbf{R}$ (or $\\theta$)**\n* **a translation vector $\\mathbf{t}$**\n\nHowever, shocker, these don't exactly correspond to the camera's rotation and translation. \n\nThe extrinsic matrix takes the form of a rigid transformation matrix: \n* a $3\\times 3$ rotation matrix in the left-block\n* a $3\\times 1$ translation column-vector in the right\n\n$$\n\\left [\n    \\begin{array}{c|c}\nR & t\n    \\end{array}\n\\right] = \n\\left [\n    \\begin{array}{ccc|c}\nr_{1,1} & r_{1,2} & r_{1,3} & t_1 \\\\\nr_{2,1} & r_{2,2} & r_{2,3} & t_2 \\\\\nr_{3,1} & r_{3,2} & r_{3,3} & t_3\n    \\end{array}\n\\right]\n$$\n\n<br>\n\nIt's common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation\n\n<br>\n\n$$\n\\begin{align}\n    \\left [\n        \\begin{array}{c|c} \n            R & \\boldsymbol{t} \\\\\n            \\hline\n            \\boldsymbol{0} & 1 \n        \\end{array}\n    \\right ] &= \n    \\left [\n        \\begin{array}{c|c} \n            I & \\boldsymbol{t} \\\\\n            \\hline\n            \\boldsymbol{0} & 1 \n        \\end{array}\n    \\right ] \n    \\times\n    \\left [\n        \\begin{array}{c|c} \n            R & \\boldsymbol{0} \\\\\n            \\hline\n            \\boldsymbol{0} & 1 \n        \\end{array}\n    \\right ] \\\\\n        &=\n\\left[ \\begin{array}{ccc|c} \n1 & 0 & 0 & t_1 \\\\\n0 & 1 & 0 & t_2 \\\\\n0 & 0 & 1 & t_3 \\\\\n  \\hline\n0 & 0 & 0 & 1\n\\end{array} \\right] \\times\n\\left[ \\begin{array}{ccc|c} \nr_{1,1} & r_{1,2} & r_{1,3} & 0  \\\\\nr_{2,1} & r_{2,2} & r_{2,3} & 0 \\\\\nr_{3,1} & r_{3,2} & r_{3,3} & 0 \\\\\n  \\hline\n0 & 0 & 0 & 1\n\\end{array} \\right] \n\\end{align}\n$$\n\n<br>\n\nThis matrix describes how to transform points in world coordinates to camera coordinates. \n* The vector $t$ can be interpreted as the position of the world origin in camera coordinates\n* The columns of $R$ represent represent the directions of the world-axes in camera coordinates.\n\n**The important thing to remember about the extrinsic matrix is that it describes how the world is transformed relative to the camera.** This is often counter-intuitive, because we usually want to specify how the camera is transformed relative to the world. \n\n---\n\nLet's briefly look into each matrix type to get a grasp of what is happening in each in laymens terms**\n\nIn 3D space, rotation can occur about the x, y, or z-axis. Such a type of rotation that occurs about any one of the axis is known as a basic or elementary rotation. Given below are the rotation matrices that can rotate a vector through an angle about any particular axis.\n\n<br>\n\n$$\n\\begin{align}\n    R_x = \\left [\n        \\begin{array}{ccc} \n            1 & 0 & 0 \\\\\n            0 & \\cos(\\boldsymbol{\\gamma}) & -\\sin(\\boldsymbol{\\gamma}) \\\\\n            0 & \\sin(\\boldsymbol{\\gamma}) & \\cos(\\boldsymbol{\\gamma}) \n        \\end{array}\n    \\right ] \n\\end{align} \\qquad\\text{This is also known as ROLL}\\quad\\text{(CC-rotation of }\\gamma\\text{ about the x-axis)}\n$$ \n\n<br>\n\n$$\n\\begin{align}\n    R_y = \\left [\n        \\begin{array}{ccc} \n            \\cos(\\boldsymbol{\\beta}) & 0 & \\sin(\\boldsymbol{\\beta}) \\\\\n            0 & 1 & 0 \\\\\n            -\\sin(\\boldsymbol{\\beta}) & 0 & \\cos(\\boldsymbol{\\beta})\n        \\end{array}\n    \\right ]\n\\end{align} \\qquad\\text{This is also known as PITCH}\\quad\\text{(CC-rotation of }\\beta\\text{ about the y-axis)}\n$$\n\n<br>\n\n$$\n\\begin{align}\n    R_z = \\left [\n        \\begin{array}{ccc} \n            \\cos(\\boldsymbol{\\alpha}) & -\\sin(\\boldsymbol{\\alpha}) & 0 \\\\\n            \\sin(\\boldsymbol{\\alpha}) & \\cos(\\boldsymbol{\\alpha}) & 0 \\\\\n            0 & 0 & 1 \n        \\end{array}\n    \\right ]\n\\end{align} \\qquad\\text{This is also known as YAW}\\quad\\text{(CC-rotation of }\\alpha\\text{ about the z-axis)}\n$$","metadata":{}},{"cell_type":"code","source":"train_df[\"t_x\"] = train_df.translation_vector.apply(lambda x: float(x.split()[0]))\ntrain_df[\"t_y\"] = train_df.translation_vector.apply(lambda x: float(x.split()[1]))\ntrain_df[\"t_z\"] = train_df.translation_vector.apply(lambda x: float(x.split()[2]))\n\ntrain_df[\"r1_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[0]))\ntrain_df[\"r2_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[1]))\ntrain_df[\"r3_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[2]))\ntrain_df[\"r1_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[3]))\ntrain_df[\"r2_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[4]))\ntrain_df[\"r3_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[5]))\ntrain_df[\"r1_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[6]))\ntrain_df[\"r2_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[7]))\ntrain_df[\"r3_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[8]))\n\nfig = px.histogram(train_df, \"t_x\", title=\"<b>Translation Value <sub>x</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Translation Value <sub>x</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\nfig = px.histogram(train_df, \"t_y\", title=\"<b>Translation Value <sub>y</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Translation Value <sub>y</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\nfig = px.histogram(train_df, \"t_z\", title=\"<b>Translation Value <sub>z</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Translation Value <sub>z</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r1_1\", title=\"<b>Rotation Value <sub>1,1</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>1,1</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r2_1\", title=\"<b>Rotation Value <sub>2,1</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>2,1</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r3_1\", title=\"<b>Rotation Value <sub>3,1</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>3,1</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r1_2\", title=\"<b>Rotation Value <sub>1,2</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>1,2</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r2_2\", title=\"<b>Rotation Value <sub>2,2</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>2,2</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r3_2\", title=\"<b>Rotation Value <sub>3,2</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>3,2</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r1_3\", title=\"<b>Rotation Value <sub>1,3</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>1,3</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\n\nfig = px.histogram(train_df, \"r2_3\", title=\"<b>Rotation Value <sub>2,3</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>2,3</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()\n\nfig = px.histogram(train_df, \"r3_3\", title=\"<b>Rotation Value <sub>3,3</sub> Coloured By Scene</b>\", color=\"scene\", log_y=True)\nfig.update_layout(\n    yaxis_title=\"<b>Number Of Images Per Bin <sub>(log-scale)</sub></b>\", xaxis_title=\"<b>Rotation Value <sub>3,3</sub></b>\",\n    legend_title=\"<b>Scene Identifier</b>\",\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.5 LOOK AT A SINGLE EXAMPLE W/ COVISIBILITY & CALIBRATION INFORMATION</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\nEXAMPLE OF HIGH COVISIBILITY\")\nplot_two_paths(\n    train_map[\"british_museum\"][\"pco_df\"][\"image_path_1\"][0], \n    train_map[\"british_museum\"][\"pco_df\"][\"image_path_2\"][0],\n    [f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[0]}\",\n     f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[0]}\"]\n)\n\nprint(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP LEFT IMAGE (ID='93658023_4980549800') WE SEE THE FOLLOWING:\")\ncal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"93658023_4980549800\"]\nfor _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n    print(arr_from_str(cal_row_from_id[_property].values[0]))\n\nprint(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP RIGHT IMAGE (ID='77723525_5227836172') WE SEE THE FOLLOWING:\")\ncal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"77723525_5227836172\"]\nfor _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n    print(arr_from_str(cal_row_from_id[_property].values[0]))\n\n    \nprint(\"\\n\\n\\n\\nEXAMPLE OF NO COVISIBILITY\")\nplot_two_paths(\n    train_map[\"british_museum\"][\"pco_df\"][\"image_path_1\"][15395],\n    train_map[\"british_museum\"][\"pco_df\"][\"image_path_2\"][15395],\n    [f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[15395]}\",\n     f\"Covisibility Between Is {train_map['british_museum']['pco_df'].covisibility[15395]}\"]\n)\n\nprint(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP LEFT IMAGE (ID='66757775_3535589713') WE SEE THE FOLLOWING:\")\ncal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"66757775_3535589713\"]\nfor _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n    print(arr_from_str(cal_row_from_id[_property].values[0]))\n\nprint(\"\\n\\n\\nIF WE EXAMINE THE CALIBRATION DATAFRAME ROW FOR THE TOP RIGHT IMAGE (ID='66747696_4734591579') WE SEE THE FOLLOWING:\")\ncal_row_from_id = train_map[\"british_museum\"][\"cal_df\"][train_map[\"british_museum\"][\"cal_df\"].image_id==\"66747696_4734591579\"]\nfor _property in [\"camera_intrinsics\", \"rotation_matrix\", \"translation_vector\"]:\n    print(f\"\\n\\n>>>>> {_property.replace('_', ' ').upper()} ARRAY <<<<<\\n\")\n    print(arr_from_str(cal_row_from_id[_property].values[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.6 PLOT EVERY SCENE ALONG WITH COVISIBILITY</h3>\n\n---","metadata":{}},{"cell_type":"code","source":"for _s in TRAIN_SCENES:\n    print(f\"\\n\\n\\nRANDOM PLOT FOR {_s} LOCATION SCENE\")\n    pco_random_plot(train_map[_s][\"pco_df\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.7 EXAMPLE MAPPING KEYPOINTS FROM ONE IMAGE TO ANOTHER (BASIC SIFT)</h3>\n\n---","metadata":{}},{"cell_type":"code","source":"# Step 1 - Pick An Image Pair And Display\nDEMO_ROW = train_map[\"taj_mahal\"][\"pco_df\"].iloc[850]\nDEMO_IMG_ID_1 = DEMO_ROW.image_id_1\nDEMO_IMG_ID_2 = DEMO_ROW.image_id_2\nDEMO_PATH_1 = DEMO_ROW.image_path_1\nDEMO_PATH_2 = DEMO_ROW.image_path_2\n\ndemo_img_1 = cv2.imread(DEMO_PATH_1)[..., ::-1]\ndemo_img_2 = cv2.imread(DEMO_PATH_2)[..., ::-1]\n\n# FM= Fundamental Matrix\nDEMO_F = arr_from_str(DEMO_ROW.fundamental_matrix)\n\nplot_two_paths(DEMO_PATH_1, DEMO_PATH_2, _titles=[f\"IMAGE ID 1: {DEMO_IMG_ID_1}\", f\"IMAGE ID 2: {DEMO_IMG_ID_2}\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################################################################\n# Step 2 - Use SIFT (One Approach) To Detect Keypoints In Images #\n##################################################################\n#\n# The task is finding the relative geometry (rotation, translation) between the two cameras.\n# You can read more about epipolar geometry here: https://en.wikipedia.org/wiki/Epipolar_geometry\n#\n# This problem is typically (but not always!) solved with sparse features.\n# Let's try using SIFT, a seminal work in computer vision (https://en.wikipedia.org/wiki/Scale-invariant_feature_transform).\n# No longer the state of the art, but still pretty solid!\n#\nn_feat = 1_000\n\n# About parameter `contrastThreshold`:\n#     - The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. \n#     - The larger the threshold, the less features are produced by the detector.\ncontrast_thresh = -10_000\n\n# About parameter `edgeThreshold`:\n#     - The threshold used to filter out edge-like features. \n#     - Note that the its meaning is different from the contrastThreshold\n#          --> i.e. the larger the edgeThreshold, the less features are filtered out (more features are retained)\nedge_thresh = -10_000\n\n# You may want to lower the detection threshold, as small images may not be able to reach the budget otherwise.\n# Note that you may actually get more than num_features features, as a feature for one point can have multiple orientations (this is rare).\nsift_detector = cv2.SIFT_create(n_feat, contrastThreshold=contrast_thresh, edgeThreshold=edge_thresh)\n\n# Leverage sift and capture desired number of keypoints and descriptors\n#  KEYPOINTS:\n#     --> The keypoint is characterized by the 2D position, scale (proportional to the \n#         diameter of the neighborhood that needs to be taken into account), \n#         orientation and some other parameters. The keypoint neighborhood is then analyzed \n#         by another algorithm that builds a descriptor (usually represented as a feature vector). \n#           - the `.pt` attribute yields a tuple indicating the position of the keypoint \n#             as measured in pixels from the top-left corner of the image (x,y) (see drawn circle center)\n#           - the `.angle` attribute gives the keypoint orientation (see drawn flag line)\n#           - the `.size` attribute gives the keypoint magnitude/diameter (see drawn circle size)\n#           - the `.response` attribute gives the keypoint detector response on the keypoint (strength of the keypoint)\n#           - the `.octave` attribute gives the pyramid octave in which the keypoint has been detected\n#\n#  DESCRIPTORS:\n#     --> A SIFT descriptor is a 3-D spatial histogram of the image gradients in \n#         characterizing the appearance of a keypoint. The gradient at each pixel \n#         is regarded as a sample of a three-dimensional elementary feature vector, \n#         formed by the pixel location and the gradient orientation. Samples are weighed \n#         by the gradient norm and accumulated in a 3-D histogram h, which \n#         (up to normalization and clamping) forms the SIFT descriptor of the region. \n#         An additional Gaussian weighting function is applied to give less importance to \n#         gradients farther away from the keypoint center. Orientations are quantized \n#         into eight bins and the spatial coordinates into four each, as follows.\n#    --> A SIFT descriptor is essentially a certain number of features describing a keypoint.\n#        In our case, the number of features is 8*4*4 or 128 features.\nkeypoints_1, descriptors_1 = extract_sift_features(demo_img_1, sift_detector, n_feat)\nkeypoints_2, descriptors_2 = extract_sift_features(demo_img_2, sift_detector, n_feat)\n\n# Each local feature contains a keypoint (xy, possibly scale, possibly orientation) and a description vector (128-dimensional for SIFT).\n#   DRAW_RICH_KEYPOINTS Description: \n#        --> For each keypoint the circle around keypoint with keypoint size and orientation will be drawn.\nimage_with_keypoints_1 = cv2.drawKeypoints(demo_img_1, keypoints_1, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.figure(figsize=(20, 15))\nplt.imshow(image_with_keypoints_1)\nplt.axis(False)\nplt.show()\n\nimage_with_keypoints_2 = cv2.drawKeypoints(demo_img_2, keypoints_2, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.figure(figsize=(20, 15))\nplt.imshow(image_with_keypoints_2)\nplt.axis(False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################################################################################\n# Step 3 - Match The Keypoints From Image 1 to Image 2 (Many Possible Approaches) #\n###################################################################################\n#\n# For each descriptor on one image, find the closest descriptor on the other image.\n# We will leverage a Brute-force descriptor matcher.\n# NOTE:\n#     --> With crossCheck=True we keep only bidirectional matches \n#         i.e. two features are nearest neighbours from A to B and also from B to A\nbfm = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Compute matches and reduce to [query|train] indexors\ncv_matches = bfm.match(descriptors_1, descriptors_2)\nmatches_before = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n\n# Convert keypoints and matches to something more human-readable.\nkp_1_pos = np.array([kp.pt for kp in keypoints_1])\nkp_2_pos = np.array([kp.pt for kp in keypoints_2])\nkp_matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n\n# Plot the brute-force matches.\n# Notice that this includes many outliers. We can filter them with a state-of-the-art RANSAC algorithm. References:\n# * https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a\n# * https://opencv.org/evaluating-opencvs-new-ransacs\nimg_matches_before = draw_cv_matches(demo_img_1, demo_img_2, kp_1_pos, kp_2_pos, matches_before)\nplt.figure(figsize=(20, 15))\nplt.title('Matches BEFORE RANSAC', fontweight=\"bold\")\nplt.imshow(img_matches_before)\nplt.axis(False)\nplt.show()\n\n\n# OpenCV gives us the Fundamental matrix after RANSAC, and a mask over the input matches. \n# The solution is clearly much cleaner, even though it may still contain outliers.\n#########################################################################################################\n# This `F` is the prediction you'll submit to the contest.\n#########################################################################################################\nF, inlier_mask = cv2.findFundamentalMat(kp_1_pos[matches_before[:, 0]], kp_2_pos[matches_before[:, 1]], \n                                        cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n                                        confidence=0.99999, maxIters=10000)\n#########################################################################################################\n\nmatches_after = np.array([match for match, is_inlier in zip(matches_before, inlier_mask) if is_inlier])\nimg_matches_after = draw_cv_matches(demo_img_1, demo_img_2, kp_1_pos, kp_2_pos, matches_after)\nplt.figure(figsize=(20, 15))\nplt.title('Matches AFTER RANSAC', fontweight=\"bold\")\nplt.imshow(img_matches_after)\nplt.axis(False)\nplt.show()\n\n# CHECK THE FIRST KEYPOINT\nmatch_query_kp_position_1 = keypoints_1[matches_after[0][0]].pt\nmatch_query_kp_position_1 = (int(round(match_query_kp_position_1[0])), \n                             int(round(match_query_kp_position_1[1])))\nmatch_train_kp_position_1 = keypoints_2[matches_after[0][1]].pt\nmatch_train_kp_position_1 = (int(round(match_train_kp_position_1[0])), \n                             int(round(match_train_kp_position_1[1])))\n_tmp_1 = cv2.circle(demo_img_1.copy(), match_query_kp_position_1, 10, (255,0,0), -1)\n_tmp_2 = cv2.circle(demo_img_2.copy(), match_train_kp_position_1, 10, (255,0,0), -1)\n\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.title(\"First Keypoint In Image 1 (Query) AFTER RANSAC\", fontweight=\"bold\")\nplt.axis(False)\nplt.imshow(_tmp_1)\n\nplt.subplot(1,2,2)\nplt.title(\"First Keypoint In Image 2 (Train) AFTER RANSAC\", fontweight=\"bold\")\nplt.axis(False)\nplt.imshow(_tmp_2)\n\nplt.tight_layout()\nplt.show()\n\n# CHECK THE SECOND KEYPOINT\nmatch_query_kp_position_2 = keypoints_1[matches_after[1][0]].pt\nmatch_query_kp_position_2 = (int(round(match_query_kp_position_2[0])), \n                             int(round(match_query_kp_position_2[1])))\nmatch_train_kp_position_2 = keypoints_2[matches_after[1][1]].pt\nmatch_train_kp_position_2 = (int(round(match_train_kp_position_2[0])), \n                             int(round(match_train_kp_position_2[1])))\n\n_tmp_1 = cv2.circle(_tmp_1, match_query_kp_position_2, 10, (0,255,0), -1)\n_tmp_2 = cv2.circle(_tmp_2, match_train_kp_position_2, 10, (0,255,0), -1)\n\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.title(\"Second Keypoint In Image 1 (Query) AFTER RANSAC\", fontweight=\"bold\")\nplt.axis(False)\nplt.imshow(_tmp_1)\n\nplt.subplot(1,2,2)\nplt.title(\"Second Keypoint In Image 2 (Train) AFTER RANSAC\", fontweight=\"bold\")\nplt.axis(False)\nplt.imshow(_tmp_2)\nplt.tight_layout()\nplt.show()\n\n# CHECK THE THIRD KEYPOINT\nmatch_query_kp_position_3 = keypoints_1[matches_after[2][0]].pt\nmatch_query_kp_position_3 = (int(round(match_query_kp_position_3[0])), \n                             int(round(match_query_kp_position_3[1])))\nmatch_train_kp_position_3 = keypoints_2[matches_after[2][1]].pt\nmatch_train_kp_position_3 = (int(round(match_train_kp_position_3[0])), \n                             int(round(match_train_kp_position_3[1])))\n\n_tmp_1 = cv2.circle(_tmp_1, match_query_kp_position_3, 10, (0,0,255), -1)\n_tmp_2 = cv2.circle(_tmp_2, match_train_kp_position_3, 10, (0,0,255), -1)\n\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.title(\"Third Keypoint In Image 1 (Query) AFTER RANSAC\", fontweight=\"bold\")\nplt.axis(False)\nplt.imshow(_tmp_1)\n\nplt.subplot(1,2,2)\nplt.title(\"Third Keypoint In Image 2 (Train) AFTER RANSAC\", fontweight=\"bold\")\nplt.axis(False)\nplt.imshow(_tmp_2)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########################\n# Step 4 - See How We Did #\n###########################\n#\n# One important caveat: the scenes were reconstructed from unstructured image collections using \n# Structure-from-Motion (http://colmap.github.io), and are not up to \"real-world\" scale (i.e. meters, or inches).\n#\n# The hosts have computed a scaling factor per scene to correct this. \n# This is necessary to compute the metric correctly.\nDEMO_SCALE_FACTOR = scale_map[\"taj_mahal\"]\nprint(f\"The Taj Mahal Scaling Factor is: {DEMO_SCALE_FACTOR}\")\n\ninlier_kp_1 = np.array([kp.pt for i, kp in enumerate(keypoints_1) if i in matches_after[:, 0]])\ninlier_kp_2 = np.array([kp.pt for i, kp in enumerate(keypoints_2) if i in matches_after[:, 1]])\n\nE, R, T = compute_essential_matrix(F, id2cal_map[DEMO_IMG_ID_1][\"camera_intrinsics\"], id2cal_map[DEMO_IMG_ID_2][\"camera_intrinsics\"], inlier_kp_1, inlier_kp_2)\nq = quaternion_from_matrix(R)\nT = T.flatten()\n\n# Get the ground truth relative pose difference for this pair of images.\nR1_gt = id2cal_map[DEMO_IMG_ID_1][\"rotation_matrix\"]\nT1_gt = id2cal_map[DEMO_IMG_ID_1][\"translation_vector\"].reshape((3, 1))\n\nR2_gt = id2cal_map[DEMO_IMG_ID_2][\"rotation_matrix\"]\nT2_gt = id2cal_map[DEMO_IMG_ID_2][\"translation_vector\"].reshape((3, 1))\n\ndR_gt = np.dot(R2_gt, R1_gt.T)\ndT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n\nq_gt = quaternion_from_matrix(dR_gt)\nq_gt = q_gt / (np.linalg.norm(q_gt) + 1e-15)\n\n# Given ground truth and prediction, compute the error for the example above.\nerr_q, err_t = compute_error_for_example(q_gt, dT_gt, q, T, DEMO_SCALE_FACTOR)\n\nprint(f'rotation_error={err_q:.02f} (deg), translation_error={err_t:.02f} (m)', flush=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"model_baseline\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"model_baseline\">\n    5&nbsp;&nbsp;MODEL BASELINE&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:12:49.194824Z","iopub.execute_input":"2022-03-09T16:12:49.195617Z","iopub.status.idle":"2022-03-09T16:12:49.202167Z","shell.execute_reply.started":"2022-03-09T16:12:49.195577Z","shell.execute_reply":"2022-03-09T16:12:49.200859Z"}}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">5.1 BASELINE USING SIMPLE SIFT+RANSAC</h3>\n\n---\n\n**Let's create a wrapper function that can take an image pair and will return an approximate F matrix so we can submit using the basic techniques illustrated by the hosts in this notebook --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?rvi=1**","metadata":{}},{"cell_type":"code","source":"INFERENCE_N_FEATURES      = 5_000\nINFERENCE_CONTRAST_THRESH = -10_000\nINFERENCE_EDGE_THRESH     = -10_000\nINFERENCE_N_OCTAVES       = 3\nDETECTOR_STYLE            = \"SIFT\" # [\"SIFT\"|\"ORB\"]\nMATCHER_STYLE             = \"BFM\"\n\n# IF USING ORB WE ALSO USE BEBLID\n#\n# BEBLID (Boosted Efficient Binary Local Image Descriptor): \n#           - A new descriptor introduced in 2020 that has been shown to improve ORB \n#             in several tasks. Since BEBLID works for several detection methods, \n#             you have to set the scale for the ORB keypoints which is 0.75~1. \nif DETECTOR_STYLE==\"SIFT\":\n    INFERENCE_DETECTOR = cv2.SIFT_create(INFERENCE_N_FEATURES, INFERENCE_N_OCTAVES, INFERENCE_CONTRAST_THRESH, INFERENCE_EDGE_THRESH)\n    INFERENCE_DESCRIPTOR = None\nelif DETECTOR_STYLE==\"ORB\":\n    INFERENCE_DETECTOR = cv2.ORB_create(INFERENCE_N_FEATURES, edgeThreshold=INFERENCE_EDGE_THRESH)\n    INFERENCE_DESCRIPTOR = cv2.xfeatures2d.BEBLID_create(0.75)\nelse:        \n    raise NotImplementedError(\"Only SIFT and ORB are Implemented So Far\")\n    \nif MATCHER_STYLE==\"BFM\":\n    INFERENCE_MATCHER = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\nelse:\n    raise NotImplementedError(\"Only BruteForce Is Implemented So Far\")\n\n# We know there are ~10000 pairs of images so ideally we want this to run pretty quick\ndef get_fundamental_matrix_v1(img_path_1, img_path_2, detector, matcher, descriptor=None, n_feat=10_000, fancy_matching=False):\n    # Step 1 - Load the Images\n    img_1 = cv2.imread(img_path_1)[..., ::-1]\n    img_2 = cv2.imread(img_path_2)[..., ::-1]\n    \n    # Step 2 - Get KPs and Descriptors (Slowest Step --> ~0.6-0.8 seconds)\n    if descriptor is None:\n        keypoints_1, descriptors_1 = extract_sift_features(img_1, detector, n_feat)\n        keypoints_2, descriptors_2 = extract_sift_features(img_2, detector, n_feat)\n    else:\n        keypoints_1, descriptors_1 = descriptor.compute(img_1, extract_keypoints(img_1, detector))\n        keypoints_1, descriptors_1 = keypoints_1[:n_feat], descriptors_1[:n_feat]\n        \n        keypoints_2, descriptors_2 = descriptor.compute(img_2, extract_keypoints(img_2, detector))\n        keypoints_2, descriptors_2 = keypoints_2[:n_feat], descriptors_2[:n_feat]\n    \n    # Step 3 - Get position information from Keypoint objects\n    kp_1_pos = np.array([kp.pt for kp in keypoints_1])\n    kp_2_pos = np.array([kp.pt for kp in keypoints_2])\n    \n    # Step 4 - Compute Matches (Before RANSAC)(2nd Slowest Step --> ~0.1-0.2 seconds)\n    \n    if not fancy_matching:\n        matches = matcher.match(descriptors_1, descriptors_2)\n        matches = np.array([[m.queryIdx, m.trainIdx] for m in matches])\n    else:\n        matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)\n        nn_matches = matcher.knnMatch(descriptors_1, descriptors_2, 2)\n        matches = np.array([[m.queryIdx, m.trainIdx] for m,n in nn_matches if m.distance<(0.8*n.distance)])\n        \n    # Step 5 - Use CV2 To Perform RANSAC and Get the Fundamental Matrix\n    F, inlier_mask = cv2.findFundamentalMat(kp_1_pos[matches[:, 0]], kp_2_pos[matches[:, 1]], \n                                            cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n                                            confidence=0.99999, maxIters=15000)\n    \n    # Step 6 - Convert our array to a flattened string \n    return \" \".join([str(x) for x in F.flatten()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">5.2 DELF BASELINE</h3>\n\n---\n\nThe DELF module takes an image as input and will describe noteworthy points with vectors.","metadata":{}},{"cell_type":"code","source":"# Define the DELF constants... shown raw below\nDELF_SCALES = tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0])\nDELF_SCORE_THRESH = tf.constant(100.0)\nDELF_MAX_FEATURES = tf.constant(2500)\n\n# Prepare an image tensor.\ntf_demo_img_1 = tf.cast(demo_img_1, tf.float32)\ntf_demo_img_2 = tf.cast(demo_img_2, tf.float32)\n\n# Instantiate the DELF module using the offline directory\nLOCAL_TFHUB_DELF_PATH = \"/kaggle/input/offline-delf-from-tfhub/delf_cache/333559635dbfa88957ed2e7ed45bdbfe3353e356\"\ndelf_module = tfhub.load(LOCAL_TFHUB_DELF_PATH).signatures['default']\n\ndelf_inputs_1 = {\n  # An image tensor with dtype float32 and shape [height, width, 3], where\n  # height and width are positive integers:\n  'image': tf_demo_img_1,\n    \n  # Scaling factors for building the image pyramid as described in the paper:\n  'image_scales': tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0]),\n    \n  # Image features whose attention score exceeds this threshold will be returned:\n  'score_threshold': tf.constant(100.0),\n    \n  # The maximum number of features that should be returned:\n  'max_feature_num': tf.constant(1000),\n}\n\n# Apply the DELF module to the inputs to get the outputs.\n#\n# NOTE: \n#  delf_outputs is a dictionary of named tensors:\n#      * delf_outputs['locations']: \n#           - a Tensor with dtype float32 and shape [None, 2],\n#           - where each entry is a coordinate (vertical-offset, horizontal-offset) \n#             in pixels from the top-left corner of the image.\n#      * delf_outputs['descriptors']: \n#.          - a Tensor with dtype float32 and shape [None, 40], \n#           - where delf_outputs['descriptors'][i] is a 40-dimensional\n#             descriptor for the image at location delf_outputs['locations'][i]\ndelf_outputs_1 = delf_module(**delf_inputs_1)\n\ndelf_inputs_2 = {\"image\":tf_demo_img_2, \n                 \"image_scales\":DELF_SCALES, \n                 \"score_threshold\":DELF_SCORE_THRESH,\n                 \"max_feature_num\":DELF_MAX_FEATURES}\ndelf_outputs_2 = delf_module(**delf_inputs_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def delf_inference(img, delf_model,\n                   _scales=(0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0),\n                   _score_thresh=100.0, _n_features=2500,):\n    tf_img = tf.cast(img, tf.float32)\n    input_map = {\"image\":tf_img, \n                 \"image_scales\":tf.constant(_scales), \n                 \"score_threshold\":tf.constant(_score_thresh),\n                 \"max_feature_num\":tf.constant(_n_features)}\n    outputs = delf_model(**input_map)\n    keypoints, descriptors = outputs[\"locations\"].numpy(), outputs[\"descriptors\"].numpy()    \n    keypoints[:, [0, 1]] = keypoints[:, [1, 0]]\n    return keypoints, descriptors\n\n\ndef use_nn_to_filter_kps(keypoints_1, keypoints_2, descriptors_1, descriptors_2, d_thresh=0.8):\n    n_kp_1, n_kp_2 = keypoints_1.shape[0], keypoints_2.shape[0]\n    \n    # Find nearest-neighbor matches using a KD tree.\n    d1_tree = cKDTree(descriptors_1)\n    _, idxs = d1_tree.query(descriptors_2, distance_upper_bound=d_thresh)\n\n    # Select feature locations for putative matches.\n    keypoints_2 = np.array([keypoints_2[i,] for i in range(n_kp_2) if idxs[i]!=n_kp_1])\n    keypoints_1 = np.array([keypoints_1[idxs[i],] for i in range(n_kp_2) if idxs[i]!=n_kp_1])\n    \n    matches = np.array([(i,i) for i in range(keypoints_1.shape[0])])\n    return keypoints_1, keypoints_2, matches\n\ndef get_fundamental_matrix_v2(img_path_1, img_path_2, detector, matcher=\"nn\", n_feat=10_000, do_plots=False):\n    # Step 1 - Load the Images\n    img_1 = cv2.imread(img_path_1)[..., ::-1]/255.\n    img_2 = cv2.imread(img_path_2)[..., ::-1]/255.\n    \n    # Step 2 - Get KPs and Descriptors\n    kp_1_pos, descriptors_1 = delf_inference(img_1, detector, _n_features=n_feat)\n    kp_2_pos, descriptors_2 = delf_inference(img_2, detector, _n_features=n_feat)\n        \n    # Step 3 - Compute Matches (Before RANSAC)(2nd Slowest Step --> ~0.1-0.2 seconds)\n    if matcher==\"nn\":\n        kp_1_pos, kp_2_pos, matches = use_nn_to_filter_kps(kp_1_pos, kp_2_pos, \n                                                           descriptors_1, descriptors_2)\n    else:\n        matches = matcher.match(descriptors_1, descriptors_2)\n        matches = np.array([[m.queryIdx, m.trainIdx] for m in matches])\n        \n    if do_plots:\n        img_matches_before = draw_cv_matches(255*img_1.copy(), 255*img_2.copy(), kp_1_pos, kp_2_pos, matches)\n        plt.figure(figsize=(20, 15))\n        plt.title('Matches BEFORE RANSAC', fontweight=\"bold\")\n        plt.imshow(img_matches_before)\n        plt.axis(False)\n        plt.show()\n        \n    # Step 4 - Use CV2 To Perform RANSAC and Get the Fundamental Matrix\n    F, inlier_mask = cv2.findFundamentalMat(kp_1_pos[matches[:, 0]], kp_2_pos[matches[:, 1]], \n                                            cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n                                            confidence=0.99999, maxIters=100_000)\n    \n    if do_plots:\n        matches = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n        img_matches_after = draw_cv_matches(255*img_1.copy(), 255*img_2.copy(), kp_1_pos, kp_2_pos, matches)\n        plt.figure(figsize=(20, 15))\n        plt.title('Matches AFTER RANSAC', fontweight=\"bold\")\n        plt.imshow(img_matches_after)\n        plt.axis(False)\n        plt.show()\n    \n    # Step 6 - Convert our array to a flattened string \n    return \" \".join([str(x) for x in F.flatten()])\n\nprint(\"\\n\\nDEMO WITH REGULAR MATCHER\\n\")\nDEMO_F = get_fundamental_matrix_v2(DEMO_PATH_1, DEMO_PATH_2, detector=delf_module, matcher=INFERENCE_MATCHER, n_feat=500, do_plots=True)\n\nprint(\"\\n\\nDEMO WITH KNN MATCHER\\n\")\nDEMO_F = get_fundamental_matrix_v2(DEMO_PATH_1, DEMO_PATH_2, detector=delf_module, matcher=\"nn\", n_feat=500, do_plots=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"model_baseline\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"submission\">\n    6&nbsp;&nbsp;SUBMISSION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n\n**For this submission I will use the KNN matcher and the newly minted DELF module**","metadata":{}},{"cell_type":"code","source":"all_test_image_paths_1 = test_df.f_path_1.tolist()\nall_test_image_paths_2 = test_df.f_path_2.tolist()\n\nif len(all_test_image_paths_1)==3:\n    ss_df[\"fundamental_matrix\"] = [get_fundamental_matrix_v2(_path_1, _path_2, detector=delf_module, matcher=\"nn\", do_plots=True) for _path_1, _path_2 in zip(all_test_image_paths_1, all_test_image_paths_2)]\nelse:\n    ss_df[\"fundamental_matrix\"] = [get_fundamental_matrix_v2(_path_1, _path_2, detector=delf_module, matcher=INFERENCE_MATCHER) for _path_1, _path_2 in zip(all_test_image_paths_1, all_test_image_paths_2)]\n    \ndisplay(ss_df)\nss_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}